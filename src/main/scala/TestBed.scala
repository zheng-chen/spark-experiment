
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.hadoop.conf.Configuration
import org.bson.BSONObject
import org.bson.BasicBSONObject

object TestBed {

  def main(args: Array[String]) {

    val sc = new SparkContext("local", "Testing program")
    
    val config = new Configuration()
    config.set("mongo.input.uri", "mongodb://127.0.0.1:27017/testdata.app.tasks")
//    config.set("mongo.input.query", "{\"_id\":{\"$oid\":\"51a67c7572338ce8f0003ef6\"}}")
//    config.set("mongo.output.uri", "mongodb://127.0.0.1:27017/beowulf.output")

    val mongoRDD = sc.newAPIHadoopRDD(config, classOf[com.mongodb.hadoop.MongoInputFormat], classOf[Object], classOf[BSONObject])

    // Input contains tuples of (ObjectId, BSONObject)
    mongoRDD.foreach(item=>{println(item._2.get("description"))})
    
//    val trans = mongoRDD.flatMap(arg => {
//      var str = arg._2.get("description").toString
//      str = str.toLowerCase().replaceAll("[.,!?\n]", " ")
//      str.split(" ")
//    });
//    
//    val countsRDD = trans
//    .map(word => (word, 1))
//    .reduceByKey((a, b) => a + b)
    
    println(mongoRDD.count());
    
    // Output contains tuples of (null, BSONObject) - ObjectId will be generated by Mongo driver if null
//    val saveRDD = countsRDD.map((tuple) => {
//      var bson = new BasicBSONObject()
//      bson.put("word", tuple._1)
//      bson.put("count", tuple._2)
//      (null, bson)
//    })
//    
//    // Only MongoOutputFormat and config are relevant
//    saveRDD.saveAsNewAPIHadoopFile("file:///bogus", classOf[Any], classOf[Any], classOf[com.mongodb.hadoop.MongoOutputFormat[Any, Any]], config)

  }
}

